### Simple and Effective Multi-Paragraph Reading Comprehension    

#### 摘要

我们考虑使神经段落级别问答模型适应到整个文档都作为输入的问题。我们提出的方法训练模型产生非常标准置信得分对于他们在单段落的结果。我们在训练时从文档中采样多个段落。使用共享标准化训练目标是模型产生全局的正确输出。我们在文档QA数据集上将这种方法与最先进的管道流训练模型。实验结果表明我们能够获得71.3$F_1$得分，之前的最好的系统只有56.7$F_1$。

#### 介绍

教机器回答任意用户产生问题是一个自然语言处理长期目标。对于问题宽范围，已存在信息检索方法能够定位文档来确定答案。但是，自动从这些文本中抽取是一个公开挑战。给定一个相关的段落的神经模型的最近成功认为神经模型有潜力成为一个关键的方法来解决这个问题。训练和测试那些将这个文档作为输入时非常计算昂贵的。所以这要求基于段落的模型来处理文档级别的输入。

对于这个任务，有两个基本的方法。管道流方法从输入文档中选择一个简单段落，然后输入到段落级别的模型中抽取答案。基于置信的方法应用模型到多个多个段落，返回带有最高置信的答案。置信方法在段落选择是上鲁棒性，但是他们要求对每个段落产生正确的置信度得分。正如我们所知，正常的，模型是很难满足的。

这篇论文中我们开始提出一个改进管道流方法获得最先进的结果。我们介绍一个方法训练模型产生每段落置信得分，我们显示怎样合并这个方法与多段落选择进一步提升性能呢。

我们的管道流方法聚焦于处理这挑战。我们提出TF-IDF启发式方法去选择那个段落训练或者测试。评价整个文档是非常昂贵的，这数据的顺序是远举例监督，不是答案区间。为了处理这产生的噪声，是模型在所有位置那些答案存在的输出边缘化。我们应用这个方法在阅读理解模型，包括自注意力，双向自注意力。

我们置信方法扩展这方法更好处理多段落设置。之前的方法训练模型在问题和段落上。作为先验包含答案。这有几个缺点：这模型没有训练产生低确信度得分的段落：那些没有包括答案的，我们处理这些问题通过从上下文中采样段落，包括段落：不包含答案，进行训练。我们使用一个共享标准化对象：段落独立处理。

#### 管道流方法

这部分我们提出一个方法来训练管道流问答系统，一个简单段落启发式从上下文中抽取，然后传给段落级别的QA模型。我们使用基于TF-IDF的段落选择方法和讨论一个概括目标函数，应该用于处理噪声监督。我们也提出一个预定义模型，整合阅读理解系统最近模型想法。

##### 段落选择

我们段落选择方法选择段落：与问题有最小的TF-IDF余弦距离值。文档频率计算使用有文档相关的段落，而不是整个语料。这个方法的优势是是否一个问题词是否经常存在，对于那些不普遍存在的给更大的权值。

##### 处理噪声标签

![1537945739809](https://github.com/jxz542189/paper_translate/raw/master/image/S_norm1.png)

图1：噪声监督导致文本区间多包含答案，但是并不存在与上下文中与问题相关，被标记为正确答案区间。

  在距离监督设置，我们标记所有文本区间，匹配答案文本作为正确。这个导致训练的模型选择不期待答案区间。图1包含一个例子。味蕾处理这个问题，我么使用概述目标函数，优化所有答案区间的概率之和。这模型我们考虑的，工作独立预测答案区间的开始和结束。因此区间开始边界的目标函数为：
$$
-log(\frac{\sum_{k \in A }{e^s_k}}{\sum_{i=1}^ne^s_i})
$$
$A$是一个标记集合：开始一个答案区间，n是上下文标记的数量，$s_i$是区间i的标量得分通过模型计算得到的。

##### 模型

![1537946734470](https://github.com/jxz542189/paper_translate/raw/master/image/S_norm2.png)

​                                                       图2：模型结构

***嵌入层***：我们嵌入字使用预先训练的字向量。我们也嵌入每个字中字符，使用20维向量。后面接有卷积神经网络和最大池化来得到每个字的字符嵌入。再输入到下一层之前整合字嵌入和字符嵌入。我们没有更新字嵌入在训练的过程中。

***预处理***：一个共享双向GRU别用于映射问题和段落嵌入到具有上下文意识的嵌入。

***注意力***：BiDAF模型的双向注意力机制用于构建一个查询意识的上下文表示。$h_i$表示上下文字i的向量，$q_j$是问题字j的向量。$n_q$和$n_c$分别表示问题和上下文长度。我们计算上下文字i和问题字j的注意力：
$$
a_{ij}=W_1 \cdot h_i +w_2 \cdot q_j +w_3 \cdot (h_i \odot q_j)
$$
计算每个上下文表示的注意力向量$c_i$:
$$
p_{ij}=\frac{e^a_{ij}}{\sum_{j=1}^{n_q}e^q_{ij}}
$$

$$
c_i=\sum_{j=1}^{n_q}q_jp_{ij}
$$

查询上下文向量$q_c$:
$$
m_i=\max_{i\leq j \leq n_q}a_{ij}
$$

$$
p_i=\frac{e^m_{i}}{\sum_{i=1}^{n_{c}}e^m_{i}}
$$

$$
q_c=\sum_{i=1}^{n_c}h_ip_i
$$

每一个标记的最终向量计算通过合并$h_i$,$c_i$,$h_i\odot c_i$以及$q_c \odot c_i$。随后输入一个线性层，带有ReLU激活函数的。

***自注意力***：残余自注意力。输入通过一个双向GRU。我们应用同样的注意力机制，现在仅仅在段落间。这里没有使用查询上下文注意力，我们设置$a_{ij}=-inf$如果$i=j$。

在这之前，我们输入整合输出通过线性层带有ReLU激活函数的。这层应用残余，所以这个输出加到输入中。

***预测***：我们模型的最后一层是双向GRU，跟随一个线性层，计算每个标记的答案开始得分。这层隐藏转态整合输入到第二个双向GRU和线性层来预测答案最后得分。softmax操作用于开始和结尾产生开始和结尾的概率。

***Dropout***：我们使用变种Dropout。

#### 置信方法

我们适应模型到多段落设置通过使用非正规化和非指数化得分到每一个区间作为模型置信度的度量。基于边界模型我们所使用的，一个区间得分是一个给定开始和结尾的得分总和。我们运行模型在每一段落和选择最高的执行度的答案区间。

使用这方法并没有改变训练方式。训练目标不要求这些置信得分座位比较在段落间。我们显示这些模型在提供置信得分上比较差。在表1给了这种现象的定性例子。

![1537950529535](https://github.com/jxz542189/paper_translate/raw/master/image/S_norm3.png)我们假设这有两个关键推理一个模型置信得分可能不是很好校准。首先，模型训练使用softmax目标，预softmax得分对于所有区间可能任意增加和减少通过一个固定值，没有改变结果softmax概率分布。第二，如果模型只看见段落包含答案，它可能成为太置信，只有效在一个答案存在的先验。

##### 共享标准化

在这个方法所有段落处理通过独立。但是，一个修改目标函数被使用：标准化在softmax操作来自相同上下文的所有段落共享，因此，从段落P的标记a开始一个答案区间的概率计算如下：
$$
\frac{e^{s_{ap}}}{\sum_{j \in P\sum_{i=1}^{n_j}e^{s_{ij}}}}
$$
其中P是一个段落集合，来自相同上下文p，$s_{ij}$作为来自段落j的标记i的得分。我们训练这个目标通过包含多个段落来自相同上下文每一小批次。

这相似简单输入模型多段落来自每个上下文整合一起，处理每个段落单独处理直到标准化步骤。这关键点：使得模型产生得分：段落间进行比较。

##### 合并

作为之前方法的替换方案，我们实验：在训练阶段连接所有段落来自相同上下文。

##### 非答案选项

我们也实验：允许模型选择一个特殊的非答案选项对于每个段落。首先，注意独立边界目标被重写：
$$
-log(\frac{e^{s_a}}{\sum_{i=1}^ne_{s_i}})-log(\frac{e_{ga}}{\sum_{j=1}^ne^{g_j}})=-log(\frac{e^{s_ag_b}}{\sum_{i=1}^n\sum_{j=1}^ne_{s_ig_j}})
$$
其中$s_j$和$g_j$是开始和结尾边界对于标记j，a和b是正确的开始和结尾标志。修改目标函数为：
$$
-log(\frac{(1-\delta)e^z+\delta e^{s_ag_b}}{e_z+\sum_{i=1}^n \sum_{j=1}^ne_{s_ig_j}})
$$
我们计算z通过添加一个额外层在我们模型的结尾。我们计算一个软注意力在区间开始得分:$p_i=\frac{e_{s_i}}{\sum_{j=1}^ne_{s_j}}$.然后隐藏状态的权重和来产生得分:$v_1=\sum_{i=1}^nh_ip_i$.我们计算第2个向量，$v_2$使用相同的方式。最后已学习注意力的异步执行自注意力层输出：
$$
a_i=w \cdot h_i
$$

$$
p_i=\frac{e_{a_i}}{\sum_{j=1}^ne_{a_j}}
$$

$$
v_3=\sum_{i=1}^nh_ip_i
$$

我们整合这三个向量，用它们作为输入到一个两层网络。

##### 