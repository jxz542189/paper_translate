### Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling 

####    摘要

通过预训练语言模型来促进自然语言处理任务做了很多努力。对不同应用带来显著提升。为了各种各样的、大规模的语言模型，充分使用差不多没有限制语料和获得语法信息是需要的。但是对于一个特定任务，只有部分信息是有用的。大规模语言模型，甚至在推理阶段，可能导致昂重的计算负担，对于大规模应用来说，消耗太多时间。这里，我们提出压缩体积大的语言模型，保留有用信息关于特定任务。模型的不同层保留不同信息，我们开发以层选择通过稀疏减少正则来模型裁剪。通过介绍稠密连接，我们能够分离任何层而不影响其他，使得浅显的宽的语言模型扩展到深度的窄的。模型训练，语言模型通过具有层意识的dropout来获得更好的鲁棒性。实验在两个基准数据集证明我们方法的有效性。

#### 介绍

最近神经网络的发展中获利，能够接触到没有限制的语料，神经语言模型能够获得一个好的困惑得分和生成高质量的句子。这些语言模型自动从大文本语料获得丰富的语法信息和模式，能够用于促进广泛的NLP应用中。

最近，很多尝试在学习上下文描述通过预训练语言模型。这些预训练层带来显著提升对于不同NLP基准，减少30%的相对误差。但是，因为语言高可变性，巨大的神经网络优先重构语言模型信息和抽取各种各样的语言信息。即使这些模型能够整合没有保留，他们仍然有昂重的计算负担在推理阶段，在实际使用上受到限制。

在这篇文章，我们目标是对终端任务以一种即插即用的方式压缩语言模型。典型，神经网络压缩方法要求预训练整个模型。但是，神经语言模型通常有RNN 组成，他们后向传播要求显著的内存比它们的推理。这成为非常繁重的，当目标任务装备成对的语言模型在两个方法来获得信息。因此，这些方法并不适合我们的场景。因此我们尝试压缩语言模型通过避免昂贵的预训练。

直观上，不同深度层获得不同级别的语法信息。同时，由于语言模型训练在任务不可知的方式，并不是所有的层和他们抽取的信息都与终端任务相关。而后，我们提出通过层选择来压缩模型，保留有用层对于大型目标任务和裁剪无关层。但是，对于广泛使用堆叠LSTM,直接裁剪任何层将消除任何子序列。为了克服这个挑战，我们介绍稠密连接。如图1所示，它允许我们分解任何层，但是保留其余的，因此创造偏置避免预训练。而且，这些连接可以扩展浅显的宽的语言模型到深的和窄的，使能一个更加优调的层选择。

进一步，我们尝试保留裁剪模型的有效性。特别地，我们修改$L_1$正则来鼓励选择权重步进稀疏并且二进制，阻止保留层连接减少。但是，我们设计一个层意识的dropout来使语言模型更加鲁棒性和更好进行层选择。

我们定义我们的模型为LD-Net，由于层选择和稠密连接形成我们裁剪方法的偏置。为了评估，我们使用LD-Net在两个序列标签基准数据集，证明已提出方法的有效性。在CoNLL03命名实体识别任务，通过整合无裁剪的语言模型，$F_1$得分从90.78$\pm$0.24%到91.86$\pm$0.15%。同时，裁剪最好的模型（92.03%）超过90% 的计算负担后，模型仍然可以后的91.84$\pm$0.14%。

![1538093676236](F:\远传\撰写论文\LD-Net1.png)

#### LD-Net

给定输入序列T，***字级别***$\{x_1,x_2,...,x_T\}$,我们用***${x_t}$***定义为$x_t$的嵌入。对于L层神经网络，我们标记第$l^{th}$层在$t^{th}$时间戳的输入和输出为***${x_{l,t}}$***和***${h_{l,t}}$***。

##### RNN和稠密连接

用函数描述RNN层：
$$
h_{l,t}=F_l(x_{l,t},h_{l,t-1})
$$
$F_l$是第$l^{th}$层的循环单元，可以任何RNN变种和vanilla LSTM在这里使用。

一个更深神经经常更有描述能力，RNN层经常堆叠在一起形成最终模型，通过设置$x_l,t=h_{l-1,t}$

。这些vanilla堆叠RNN模型，遭受一些问题例如梯队消失，很难训练深层模型。

最近，稠密连接和残差连接提出来处理这些问题。特别，稠密连接添加任何层直接连接到所有子序列层，如图1，第$l^{th}$层的输入由 所有前面层的源输入和输出组成：
$$
x_{l,t}=[x_t,h_{1,t},...,h_{l-1,t}]
$$
相似，L层RNN的最终输出是$h_t=[x_t,h_{1,t},...,h_{L,t}]$.使用稠密连接，我们分解任何单层没有排除它的子序列层。在计算机视觉中已存在实践证明这样连接可以引导深度的窄的神经网络和分布式参数到不同的层。而且，语言模型不同层通常获得不同级别的语法信息。但是，我们能够压缩语言模型对于特定任务通过裁剪不相关的或者不重要的层。

##### 语言模型

语言模型目标是序列生成。正常滴，序列$\{x_1,...,x_T\}$的生成概率定义前向方式:
$$
p(x_1,...,x_T)=\prod_{t=1}^Tp(x_t|x_1,...,x_{t-1})
$$
$p(x_t|x_1,...,x_{t-1})$计算基于RNN的输出$h_t$。因为稠密连接，$h_t$组成不同层的输出，设计去获得不同级别的语言信息。类似于DenseNet中的瓶颈层。我们添加层同一信息。我们添加一个映射层带有ReLU激活函数的：
$$
h_t^*=ReLU(W_{proj}\cdot h_t + b_{proj})
$$
基于$h_t^*$,它的直觉是计算$p(x_t|x_1,...,x_{t-1})$通过softmax函数：softmax($W_{out}\cdot h_t^*+b$).

训练语言模型不需要其他的，只需要原始文本，对语料没有限制。有些技术被提出处理这个问题，包括adaptive softmax,slim词嵌入，采样softmax和噪声对比评估。这里选择adaptive softmax.

##### 上下文描述

预训练的语言模型能够精确地描述文本生成，它们能够用于其他任务的抽取信息和重构特征。这些特征被定义为上下文描述，被证明是游泳的。为了获得两个方向的信息，我们不仅使用前向而且使用后向。在后向语言模型RNN$x_t$的输出记为$h_t^r$：
$$
p(x_1,...,x_n)=\prod_{t=1}^Tp(x_t|x_{t+1},...,x_T)
$$
语言模型最终输出将与目标词的描述相同。因此，它可能不包含上下文信息。同时，稠密连接RNN的输出包括每一层输出，因此汇总所有抽取特征。由于$h_t$的维度可能对于终端任务太大。我添加一个非线性转换来计算上下文描述$r_t$:
$$
r_t=ReLU(W_{cr}\cdot [h_t,h_t^r+b_{cr}])
$$
我们提出的方法有些类似于ELMo，ELMo设计vanilla堆叠RNN，尝试计算不同层输出权值平均作为上下文描述。我们方法从稠密连接和它的窄结构中获利，能够直接合并不同层的输出。它并不认为不同层的输出在相同的空间。

##### 层选择

典型模型压缩方法要求预训练或者梯度计算。对于结对的语言模型，这些模型要求更多的计算资源与语言模型的训练，并不适合我们的场景。

从稠密连接获利，我们能够训练深度和窄的网络。而且，我们分解它的一层没排除它的所有子序列层。由于神经网络的不同层能够获得不同语法信息。只有一些或者有用对于一个特定任务。结果，我们尝试压缩这些模型通过任务引导的层选择。对于第i层，我们介绍一个二进制掩码$z_i \in \{0,1\}$.重新计算$h_{l,t}$：
$$
h_{l,t}=z_i \cdot F_l(x_{l,t}, h_{l,t-1})
$$

##### 层意识Dropout

我们自定义层意识裁剪的正则项，阻止保留层的链接减少。我们尝试进一步保留压缩模型的有效性。特别地，我们选择准备语言模型对于裁剪的输入数据，使它们更加鲁棒性。

我们执行语言模型的训练，带有层意识的dropout。如图3，层的一个随机部分，在每一批次随机减少掉。以减少层的输出将不输入到它们的子序列循环层，但是要输入到映射层用来预测下一个字。

换句话说，dropout只应用于循环层输入，用于模拟裁剪输入没有完整移除任何层。

![1538113439152](F:\远传\撰写论文\LD_Net2.png)

#### 序列标签

我们将介绍我们的序列标签框架。

##### 神经架构

字符级别描述已经成为必要的组件。我们使用LSTM将字符级别输入以上下文意识的方式。标记它的输出对于$x_t$作为$c_t$。类似于上下文描述，$c_t$有更多的维度比$w_t$.为了整合他们在一起，我们设置输出维度为公式5作为$w_t$的维度，映射$c_t$作为一个新空间有同样的维度。我们标记已映射的字符级别描述为$c_t*$。

![1538114480440](F:\远传\撰写论文\LD_Net3.png)

映射后，这些向量整合为$v_t=[c_t*;r_t;w_t]$，进一步输入到字级别的LSTM。模型定义$y=\{y_1,...,y_T\}$的生成概率为:
$$
p(y|U)=\frac{\prod_{t=1}^T\phi(y_{t-1},y_t,u_t)}{\sum_{\hat y \in Y(U)}\prod_{t=1}^T\phi(\hat{y_{t-1} },\hat  y_t , u_t)}
$$
其中$\hat y=\{\hat y_1, ...,\hat y_T\}$是类的标签序列，$Y(U)$是一个类标签序列。$\phi(y_{t-1},y_t,u_t)$定义为exp($W_{y_t}u_t+b_{y_{t-1},y_t}$)。

##### 模型训练和推理

我们使用下面负对数似然:
$$
L=-\sum_U log p(y|U)
$$
对于测试或者解码，我们求最佳的$y^*$:
$$
y^*=\arg\max_{y \in Y(U)} p(y|U)
$$
***模型训练***。我们设置$\lambda_0$为0，并且优化损失函数没有附加正则化。这一步，我们执行优化使用带有动量的随机梯度。dropout将加入到结对的语言模型和序列标签模型。

***模型裁剪***。我们执行裁剪基于检查点：有最好的性能在训练的时候。我们设置$\lambda_0$为非0值，优化$\min L+\lambda_0R_3$.



