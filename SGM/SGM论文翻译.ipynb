{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGM: Sequence Generation Model for Multi-Label Classification    \n",
    "\n",
    "#### 摘要\n",
    "\n",
    "​     多标签分类在自然语言处理中还是一个重要挑战任务，远比单标签分类复杂，并且标签之间还相互关联。现存方法忽略标签的关联性。此外，文本不同部分对于预测不同标签贡献不同。已存在模型没有考虑这个问题。在本文，我们提出将多标签分类任务作为一个序列生成问题。应用一个带有新颖的解码结构的序列生成模型来解决它。拓展实验结果显示我们提出的模型超过之前工作。进一步分析实验结果证明已提出方法不仅获得标签的相关信息，而且在预测不同标签自动选择最大信息信息词。\n",
    "\n",
    "#### 介绍\n",
    "\n",
    "​    多标签分类是自然语言领域中一个重要任务，使用在很多真实世界场景中，例如文本分类，标签推荐，信息检索等等。\n",
    "\n",
    "   在本文，受到序列到序列模型在机器翻译，抽象摘要，风格转换等领域的巨大成功激励，我们提出一个带有新颖解码的序列生成模型。提出的序列生成模型由基于注意力机制的解码器。解码器使用LSTM顺序生成标签，基于之前已预测标签来预测下一个标签。已提出模型考虑标签相关性，通过LSTM结构处理标签序列依赖。当模型用于预测不同标签注意力机制考虑文本不同部分作用。此外，带有全局嵌入的解码结构被提出通过整合整体信息来提升模型性能。\n",
    "\n",
    "​    本文贡献点，如下：\n",
    "\n",
    "​    我们提出将多标签分类任务作为一个序列生成问题，并且将标签关联关系加入考虑中。\n",
    "\n",
    "   我们提出带有新颖解码结构的序列生成模型，当预测不同标签时不仅获得标签关联关系，而且自动选择最大信息字。\n",
    "\n",
    "   拓展实验结果显示我们提出方法超过基线。进一步分析显示已提出方法在关联描述上是有效的。\n",
    "\n",
    "#### 已提出方法\n",
    "\n",
    "##### 综述\n",
    "\n",
    "   首先，我们定义一些概念和描述多分类任务。给定标签空间有$L$个标签,$\\mathcal{L}=\\{l_1,l_2,...,l_L\\}$,文本序列$x$包含m个字。从序列生成角度，多标签分类任务被建模为寻找一个最优标签序列${y^*}$,最大化条件概率分布$p{(y|x)}$:\n",
    "\n",
    "\n",
    "$$\n",
    "p(y|x) = \\prod_{i=1}^np(y_i|y_1,y_2,...,y_{i-1},x)\n",
    "$$\n",
    "模型结构如下图1：\n",
    "\n",
    "![1537587192595](https://github.com/jxz542189/paper_translate/raw/master/image/SGM1.png)\n",
    "\n",
    "​        图1 已提出模型整体结构，MS表示masked softmax层，GE表示全局嵌入\n",
    "\n",
    "​    首先，我们根据在训练集中标签的频率对每个样本的标签进行排序。高频率的放置在前面。此外，添加bos和eos符号添加到标签序列的头部和尾部。\n",
    "\n",
    "   文本序列${x}$被编码为隐藏状态通过时间步注意力机制来整合为上下文向量。解码器将上下文向量$c_{t}$,解码器上一个隐藏状态$s_{t-1}$以及嵌入向量$g(y_{t-1})$作为输入来产生隐藏状态$s_t$，最后，掩饰softmax层用于概率分布$y_t$输出。\n",
    "\n",
    "##### 序列生成\n",
    "\n",
    "由基于注意力的编码器和解码器组成。\n",
    "\n",
    "***Encoder***：$(\\omega_1,\\omega_2,...,\\omega_m)$表示句子，有m个字，$\\omega_i$是第i个字的one-hot表示。通过嵌入矩阵将$\\omega_i$编码为稠密嵌入向量$x_i$。这里使用BiLSTM进行计算每个字的隐藏状态:\n",
    "$$\n",
    "\\vec{h_i}=\\vec{LSTM}(\\vec{h_{i-1},x_i})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overleftarrow{h_i}=\\overleftarrow{LSTM}(\\overleftarrow{h_{i+1}}, x_i)\n",
    "$$\n",
    "\n",
    "  获得第i个字最后隐藏描述：\n",
    "$$\n",
    "h_i=[\\vec{h_i};\\overleftarrow{h_i}]\n",
    "$$\n",
    "***注意力***：注意力机制产生一个上下文向量，聚焦在文本序列不同部分，整合这些信息字的隐藏描述，注意力机制赋予权值$\\alpha_{ti}$给时间步t的第i个字：\n",
    "$$\n",
    "e_{ti}=v_{a}^Ttanh(W_as_t+U_ah_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{ti}=\\frac{exp(e_{ti})}{\\sum_{j=1}^mexp(e_{tj})}\n",
    "$$\n",
    "最后上下文向量$c_t$将被传递时间步t的解码器：\n",
    "$$\n",
    "c_t=\\sum_{i=1}^m\\alpha_{ti}h_i\n",
    "$$\n",
    "***解码器***：在时间步t的解码器隐藏状态$s_t$如下计算：\n",
    "$$\n",
    "s_t=LSTM(s_{t-1},[g(y_{t-1});c_{t-1}])\n",
    "$$\n",
    "其中$g(y_{t-1})$表示基于分布$y_{t-1}$中概率最高的标签的嵌入。$y_{t-1}$如下计算：\n",
    "$$\n",
    "o_t=W_of(W_ds_t+V_dc_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t=softmax(o_t+I_t)\n",
    "$$\n",
    "$I_t$是一个掩饰向量用于阻止解码器预测重复标签：\n",
    "$$\n",
    "(I_t)_i=\\left\\{\\frac{-\\infty \\quad 如果l_i已经在t-1时间步预测时}{0\\quad 其他}\\right\\}\n",
    "$$\n",
    "训练阶段损失函数使用交叉损失函数。\n",
    "\n",
    "##### 全局嵌入\n",
    "\n",
    "前面预测模型是下一个标签预测基于之前标签，这导致之前预测错误会将继承，这称为爆炸偏置。为了解决爆炸偏置这个问题，本文提出一个新的解码结构，嵌入向量$g(y_{t-1})$在时间步t获得描述在时间步(t-1)所有整体信息。受到高速公路的适应门激励，这里本文介绍全局嵌入，定义e为分布$y_{t-1}$的最高概率的标签的嵌入。$\\tilde{e}$在时间步t的平均权重嵌入，通过如下计算：\n",
    "$$\n",
    "\\tilde{e}=\\sum_{i=1}^Ly_{t-1}^{(i)}e_i\n",
    "$$\n",
    "其中$y_{t-1}^{(i)}$表示$y_{t-1}$的第i个元素。全局嵌入$g(y_{t-1})$通过如下获得:\n",
    "$$\n",
    "g(y_{t-1})=(1-H)\\odot e + H\\odot \\tilde{e}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H=W_1e+W_2\\tilde{e}\n",
    "$$\n",
    "\n",
    "全局嵌入$g(y_{t-1})$是个优化整合源嵌入和权值平均嵌入。$y_{t-1}$包含所有可能信息。通过考虑每个标签，可以减少由于之前时间步预测错误的标签。\n",
    "\n",
    "#### 实验部分\n",
    "\n",
    "使用指标：汉明损失，$Micro-F_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
